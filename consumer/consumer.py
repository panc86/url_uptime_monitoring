import logging
import os
from sqlalchemy import create_engine
from sqlalchemy import Column, String, Integer, Float, Date
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from kafka import KafkaConsumer
import json


# initialize logger
logger = logging.getLogger("consumer")


# RDBMS blueprints for DB creation
base = declarative_base()


# DB model
class Message(base):
    __tablename__ = 'messages'

    id = Column(Integer, primary_key=True) # autogenerated at insert
    #created_at = Column(Date())
    status_code = Column('status_code', Integer())
    elapsed_seconds = Column('elapsed_seconds', Float())
    regex_matches = Column('regex_matches', Integer())
    regex_pattern = Column('regex_pattern', String(200))

    def __repr__(self):
        return f'<Message: {self.id}>'


def setup_db(uri):
    """
    Create SQLAlchemy DB engine and return a DB session
    """
    db = create_engine(uri)
    base.metadata.create_all(db)
    logging.debug('created DB engine')
    return db


def init_session(db):
    """
    Initialize SQLAlchemy DB session to enable DB operations
    """
    Session = sessionmaker(db)
    logging.debug('created DB session')
    return Session()


def create_message_model(payload: dict):
    """
    Transform dictionary into Message SQLAlchemy Model
    """
    return Message(**payload)


def insert_to_db(message: Message, session: Session):
    """
    Insert Message into DB via session
    """
    session.add(message)
    session.commit()


def init_consumer_client(host: str, topic: str):
    """
    Kafka Consumer client instance retrieves logs from the Kafka
    Producer and allows to iterate over them.
    """
    return KafkaConsumer(
        topic,
        api_version=(2,0,1),
        bootstrap_servers=[host],
        value_deserializer=lambda x: json.loads(x.decode('utf-8')),
        auto_offset_reset='earliest',
        enable_auto_commit=True,
        group_id='my-group'
    )


def main(args):
    from argparse import ArgumentParser
    parser = ArgumentParser(description="Consume messangers from Kafka Broker.")
    parser.add_argument("--host", required=True, help="The bootstrap server address the Kafka Producer initially connects to.")
    parser.add_argument("--topic", required=True, type=str, help="The Kafka topic ID.")
    parser.add_argument("--uri", required=True, type=str, help="Database URI.")
    args = parser.parse_args(args)

    if int(os.environ['DEBUG']) == 1:
       # set level of root logger
       logging.getLogger().setLevel(logging.DEBUG)

    # Create Kafka Consumer client instance. It retrieves logs from the Kafka
    # Producer and allows to iterate over them. We then create DB entries
    client = init_consumer_client(args.host, args.topic)

    # Create DB
    db = setup_db(args.uri)
    # Initialize DB session
    session = init_session(db)
    # iter incoming messages
    for msg in client:
        insert_to_db(create_message_model(msg.value), session)
        logging.info('db insertion completed')


if __name__ == "__main__":
    import sys
    logging.basicConfig(format='[%(asctime)s] %(levelname)s: %(message)s', level=logging.INFO)
    main(sys.argv[1:])
