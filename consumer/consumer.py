from sqlalchemy import create_engine
from sqlalchemy import Column, String, Integer, Float, Date
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from kafka import KafkaConsumer
import json
import logging


# initialize logger
logger = logging.getLogger("consumer")
logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)


# RDBMS blueprints for DB creation
base = declarative_base()

# DB model
class Message(base):
    __tablename__ = 'messages'
    #created_at = Column(Date())
    id = Column(Integer, primary_key=True) # autogenerated at insert
    status_code = Column('status_code', Integer())
    elapsed_seconds = Column('elapsed_seconds', Float())
    regex_matches = Column('regex_matches', Integer())
    regex_pattern = Column('regex_pattern', String(200))

    def __repr__(self):
        return f'<Message: {self.id}>'


# Create RDBMS engine and architecture template
db_uri = "postgres://user:password@postgres:5432/database"
db = create_engine(db_uri)
# Establish a new session and create DB structure
Session = sessionmaker(db)
session = Session()
base.metadata.create_all(db)

# The Kafka Consumer retrieves logs from the Kafka
# Producer and allows to iterate them. We then create DB entries
consumer = KafkaConsumer(
    'test',
    api_version=(2,0,1),
    bootstrap_servers=['172.17.0.1:9092'],
    value_deserializer=lambda x: json.loads(x.decode('utf-8')),
    auto_offset_reset='earliest',
    enable_auto_commit=True,
    group_id='my-group'
)

for m in consumer:
    db_entry = Message(**m.value)
    session.add(db_entry)
    session.commit()
    logger.info(f'{repr(db_entry)} inserted to DB')
